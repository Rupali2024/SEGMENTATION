{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Unzip dataset**"
      ],
      "metadata": {
        "id": "zeVCiPuvfhDa"
      },
      "id": "zeVCiPuvfhDa"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/DLINKNET_DATA.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T39e0ZgPfeSN",
        "outputId": "ab34eba2-d7bd-41c4-e26e-a865f029143a"
      },
      "id": "T39e0ZgPfeSN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/DLINKNET_DATA.zip\n",
            "   creating: DLINKNET_DATA/\n",
            "   creating: DLINKNET_DATA/masked/\n",
            "  inflating: DLINKNET_DATA/masked/Abyssinian_172.png  \n",
            "  inflating: DLINKNET_DATA/masked/Abyssinian_174.png  \n",
            "  inflating: DLINKNET_DATA/masked/Abyssinian_173.png  \n",
            "  inflating: DLINKNET_DATA/masked/Abyssinian_175.png  \n",
            "  inflating: DLINKNET_DATA/masked/Abyssinian_176.png  \n",
            "   creating: DLINKNET_DATA/images/\n",
            "  inflating: DLINKNET_DATA/images/Abyssinian_174.png  \n",
            "  inflating: DLINKNET_DATA/images/Abyssinian_173.png  \n",
            "  inflating: DLINKNET_DATA/images/Abyssinian_172.png  \n",
            "  inflating: DLINKNET_DATA/images/Abyssinian_176.png  \n",
            "  inflating: DLINKNET_DATA/images/Abyssinian_175.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **req. params**"
      ],
      "metadata": {
        "id": "Pzz3PaDnflaR"
      },
      "id": "Pzz3PaDnflaR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ac14a50"
      },
      "outputs": [],
      "source": [
        "# Use Google Colab\n",
        "use_google_colab = True\n",
        "# Process the training dataset\n",
        "training_data_processing = True\n",
        "# Train the model\n",
        "model_training = True\n",
        "# Validation the model\n",
        "model_validation = False\n",
        "# Load the model from your Google Drive or local file system\n",
        "model_loading = False"
      ],
      "id": "7ac14a50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05dfeb22"
      },
      "outputs": [],
      "source": [
        "if use_google_colab:\n",
        "    from google.colab import drive\n",
        "    from google.colab import files\n",
        "    # Mount your Google Drive on your runtime\n",
        "    # drive.mount('/content/gdrive')"
      ],
      "id": "05dfeb22"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libs**"
      ],
      "metadata": {
        "id": "-O8b0_I9fsaU"
      },
      "id": "-O8b0_I9fsaU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e03cc21e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "e03cc21e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Paths req.**"
      ],
      "metadata": {
        "id": "OfLPNlPsfwi4"
      },
      "id": "OfLPNlPsfwi4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5JSCfmd7YW_"
      },
      "outputs": [],
      "source": [
        "if use_google_colab:\n",
        "    path_training = '/content/DLINKNET_DATA'\n",
        "    path_testing = '/content/DLINKNET_DATA/images/'\n",
        "    path_data = '/content/gd/data/'\n",
        "    path_model = '/content/gd/models/'\n",
        "else:\n",
        "    path_training = 'training/'\n",
        "    path_testing = 'test_set_images/'\n",
        "    path_data = 'data/'\n",
        "    path_model = 'models/'"
      ],
      "id": "z5JSCfmd7YW_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b12b480d"
      },
      "source": [
        "# **Get Device for Training**"
      ],
      "id": "b12b480d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db1c96b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e890f41-05b3-4985-eeb1-4fcbdcd53c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Utilize CPUs for computation.\n"
          ]
        }
      ],
      "source": [
        "# Determine if your system supports CUDA\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if cuda_available:\n",
        "    print('CUDA is available. Utilize GPUs for computation')\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('CUDA is not available. Utilize CPUs for computation.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "id": "db1c96b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37344ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6f2530-0735-480b-a959-fd0f9ff9cffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# Get the GPU information\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "id": "37344ed4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd0e4b0"
      },
      "source": [
        "# **Define the Neural Network**"
      ],
      "id": "dcd0e4b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00bec1a0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "UNet.py - Define the neural network for UNet.\n",
        "\"\"\"\n",
        "\n",
        "class up_sampling(nn.Module):\n",
        "    # Instantiate all the modules\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(up_sampling, self).__init__()\n",
        "        self.upsampling = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    # Define the block structure\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        up_sampling's forward function.\n",
        "        Args:\n",
        "            x (tensor): input tensor\n",
        "        Returns:\n",
        "            x (tensor): the output of this block after processing\n",
        "        \"\"\"\n",
        "        x = self.upsampling(x)\n",
        "        return x\n",
        "\n",
        "class UNet_unit(nn.Module):\n",
        "    # Instantiate all the modules\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet_unit, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    # Define the block structure\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        UNet_unit's forward function.\n",
        "        Args:\n",
        "            x (tensor): input tensor\n",
        "        Returns:\n",
        "            x (tensor): the output of this block after processing\n",
        "        \"\"\"\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    # Instantiate all the modules\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        # contracting path (left side)\n",
        "        self.contracting_level1 = UNet_unit(3, 64)\n",
        "        self.maxpooling_level1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_level2 = UNet_unit(64, 128)\n",
        "        self.maxpooling_level2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_level3 = UNet_unit(128, 256)\n",
        "        self.maxpooling_level3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_level4 = UNet_unit(256, 512)\n",
        "        self.maxpooling_level4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # bridge\n",
        "        self.bridge_level5 = UNet_unit(512, 1024)\n",
        "\n",
        "        # expansive path (right side)\n",
        "        self.upconv_level6 = up_sampling(1024, 512)\n",
        "        self.expansive_level6 = UNet_unit(1024, 512)\n",
        "        self.upconv_level7 = up_sampling(512, 256)\n",
        "        self.expansive_level7 = UNet_unit(512, 256)\n",
        "        self.upconv_level8 = up_sampling(256, 128)\n",
        "        self.expansive_level8 = UNet_unit(256, 128)\n",
        "        self.upconv_level9 = up_sampling(128, 64)\n",
        "        self.expansive_level9 = UNet_unit(128, 64)\n",
        "\n",
        "        # output\n",
        "        self.output_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Define the network structure\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        UNet's forward function.\n",
        "        Args:\n",
        "            x (tensor): input tensor\n",
        "        Returns:\n",
        "            output (tensor): the output of this model after processing\n",
        "        \"\"\"\n",
        "        # contracting path (left side)\n",
        "        # contracting path: level 1\n",
        "        level1_out = self.contracting_level1(x)\n",
        "        # contracting path: level 2\n",
        "        level1_max = self.maxpooling_level1(level1_out)\n",
        "        level2_out = self.contracting_level2(level1_max)\n",
        "        # contracting path: level 3\n",
        "        level2_max = self.maxpooling_level2(level2_out)\n",
        "        level3_out = self.contracting_level3(level2_max)\n",
        "        # contracting path: level 4\n",
        "        level3_max = self.maxpooling_level3(level3_out)\n",
        "        level4_out = self.contracting_level4(level3_max)\n",
        "\n",
        "        # bridge: level 5\n",
        "        level4_max = self.maxpooling_level4(level4_out)\n",
        "        level5_out = self.bridge_level5(level4_max)\n",
        "\n",
        "        # expansive path (right side)\n",
        "        # expansive path: level 6\n",
        "        level6_ups = self.upconv_level6(level5_out)\n",
        "        level6_cat = torch.cat([level4_out, level6_ups], dim=1)\n",
        "        level6_out = self.expansive_level6(level6_cat)\n",
        "        # expansive path: level 7\n",
        "        level7_ups = self.upconv_level7(level6_out)\n",
        "        level7_cat = torch.cat([level3_out, level7_ups], dim=1)\n",
        "        level7_out = self.expansive_level7(level7_cat)\n",
        "        # expansive path: level 8\n",
        "        level8_ups = self.upconv_level8(level7_out)\n",
        "        level8_cat = torch.cat([level2_out, level8_ups], dim=1)\n",
        "        level8_out = self.expansive_level8(level8_cat)\n",
        "        # expansive path: level 9\n",
        "        level9_ups = self.upconv_level9(level8_out)\n",
        "        level9_cat = torch.cat([level1_out, level9_ups], dim=1)\n",
        "        level9_out = self.expansive_level9(level9_cat)\n",
        "\n",
        "        # output\n",
        "        output = self.output_conv(level9_out)\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output"
      ],
      "id": "00bec1a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68b4d932"
      },
      "source": [
        "# **Create an Instance of the Neural Network**"
      ],
      "id": "68b4d932"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac408fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e82a73e-f2d4-4759-a4d3-6aa3aeaf61ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet(\n",
            "  (contracting_level1): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (maxpooling_level1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (contracting_level2): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (maxpooling_level2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (contracting_level3): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (maxpooling_level3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (contracting_level4): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (maxpooling_level4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bridge_level5): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv_level6): up_sampling(\n",
            "    (upsampling): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (expansive_level6): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv_level7): up_sampling(\n",
            "    (upsampling): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (expansive_level7): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv_level8): up_sampling(\n",
            "    (upsampling): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (expansive_level8): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv_level9): up_sampling(\n",
            "    (upsampling): Sequential(\n",
            "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (expansive_level9): UNet_unit(\n",
            "    (block): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (output_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = UNet()\n",
        "if cuda_available:\n",
        "    # Move the model to GPU\n",
        "    model.cuda()\n",
        "print(model)"
      ],
      "id": "ac408fb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51deced5"
      },
      "source": [
        "# **Load and Process the Training Dataset**"
      ],
      "id": "51deced5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75a606eb"
      },
      "outputs": [],
      "source": [
        "# The resolution of resized training images and the corresponding masks\n",
        "training_resize = 384\n",
        "# The number of resized training pairs used for data augmentation\n",
        "training_number = 3\n",
        "# The resolution of resized testing images\n",
        "testing_resize = int(608 * training_resize / 500)\n",
        "if testing_resize % 2 == 1:\n",
        "    testing_resize += 1"
      ],
      "id": "75a606eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ee7b917"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import scipy.ndimage\n",
        "from tqdm import tqdm\n",
        "\n",
        "def training_data_loading(path_training='linknet/', training_resize=384, training_number=100):\n",
        "    \"\"\"\n",
        "    Load and generate the resized training dataset and validation dataset.\n",
        "\n",
        "    Args:\n",
        "        path_training (str): the location in your Google Drive or local file system\n",
        "        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs)\n",
        "        training_number (int): the number of resized training pairs used for data augmentation\n",
        "\n",
        "    Returns:\n",
        "        images_training, labels_training (numpy): the resized training dataset\n",
        "        images_validation, labels_validation (numpy): the resized validation dataset\n",
        "    \"\"\"\n",
        "    # Determine the number of images available in the dataset\n",
        "    image_files = sorted([f for f in os.listdir(os.path.join(path_training, 'images')) if f.endswith('.png')])\n",
        "    num_images = len(image_files)\n",
        "    print(image_files)\n",
        "\n",
        "    # Initialize arrays for storing images and labels\n",
        "    images_loading = np.empty(shape=(num_images, 3, training_resize, training_resize))\n",
        "    labels_loading = np.empty(shape=(num_images, 1, training_resize, training_resize))\n",
        "\n",
        "    for index, filename in enumerate(image_files):\n",
        "        img_path = os.path.join(path_training, 'images', filename)\n",
        "        mask_path = os.path.join(path_training, 'masked', filename)\n",
        "\n",
        "        # Load image and label\n",
        "        image = np.array(Image.open(img_path)).astype(float) / 255\n",
        "        label = np.array(Image.open(mask_path)).astype(float) / 255\n",
        "\n",
        "        # Resize images and labels\n",
        "        image = resize(image, (training_resize, training_resize, 3), mode='reflect')\n",
        "        label = resize(label, (training_resize, training_resize, 1), mode='reflect')\n",
        "\n",
        "        # Transpose to (C, H, W) format\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        label = np.transpose(label, (2, 0, 1))\n",
        "\n",
        "        images_loading[index] = image\n",
        "        labels_loading[index] = label\n",
        "\n",
        "    # Permute the resized dataset randomly\n",
        "    permuted_sequence = np.random.permutation(num_images)\n",
        "    images_loading = images_loading[permuted_sequence]\n",
        "    labels_loading = labels_loading[permuted_sequence]\n",
        "\n",
        "    # Generate the training and validation datasets\n",
        "    images_training = images_loading[:training_number]\n",
        "    labels_training = labels_loading[:training_number]\n",
        "    images_validation = images_loading[training_number:]\n",
        "    labels_validation = labels_loading[training_number:]\n",
        "\n",
        "    return images_training, labels_training, images_validation, labels_validation\n",
        "\n",
        "def training_data_augmentation(images_training, labels_training, rotations, flips, shifts, training_resize=384):\n",
        "    \"\"\"\n",
        "    Generate the augmented training dataset.\n",
        "\n",
        "    Args:\n",
        "        images_training, labels_training (numpy): the resized training dataset\n",
        "        rotations (list): the parameters for rotating resized training images and their corresponding masks\n",
        "        flips (list): the parameters for flipping rotated training pairs\n",
        "        shifts (list): the parameters for shifting flipped training pairs\n",
        "        training_resize (int): the resolution of resized training pairs\n",
        "\n",
        "    Returns:\n",
        "        images_augmented, labels_augmented (numpy): the augmented training dataset\n",
        "    \"\"\"\n",
        "    num_rota = len(rotations)\n",
        "    num_flip = len(flips)\n",
        "    num_shft = len(shifts)\n",
        "\n",
        "    num_training = images_training.shape[0]\n",
        "    num_augmented = num_training * num_rota * num_flip * num_shft\n",
        "    images_augmented = np.empty(shape=(num_augmented, 3, training_resize, training_resize))\n",
        "    labels_augmented = np.empty(shape=(num_augmented, 1, training_resize, training_resize))\n",
        "\n",
        "    counter = 0\n",
        "    for index in tqdm(range(num_training)):\n",
        "        image = np.transpose(images_training[index], (1, 2, 0))\n",
        "        label = np.transpose(labels_training[index], (1, 2, 0))\n",
        "        for rota in rotations:\n",
        "            for flip in flips:\n",
        "                for shft in shifts:\n",
        "                    # Rotate a resized training pair\n",
        "                    image_rota = scipy.ndimage.rotate(image, rota, reshape=False, mode='reflect')\n",
        "                    label_rota = scipy.ndimage.rotate(label, rota, reshape=False, mode='reflect')\n",
        "\n",
        "                    # Flip the rotated training pair\n",
        "                    if flip == 'original':\n",
        "                        image_flip = image_rota\n",
        "                        label_flip = label_rota\n",
        "                    else:\n",
        "                        image_flip = flip(image_rota)\n",
        "                        label_flip = flip(label_rota)\n",
        "\n",
        "                    # Shift the flipped training pair\n",
        "                    shft_H = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n",
        "                    shft_W = np.random.uniform(low=shft[0], high=shft[1], size=1)[0]\n",
        "                    image_shft = scipy.ndimage.shift(image_flip, (shft_H, shft_W, 0), mode='reflect')\n",
        "                    label_shft = scipy.ndimage.shift(label_flip, (shft_H, shft_W, 0), mode='reflect')\n",
        "\n",
        "                    images_augmented[counter] = np.clip(np.transpose(image_shft, (2, 0, 1)), 0, 1)\n",
        "                    labels_augmented[counter] = np.transpose(label_shft, (2, 0, 1)) > 0.3\n",
        "                    counter += 1\n",
        "\n",
        "    # Permute the augmented training dataset randomly\n",
        "    permuted_sequence = np.random.permutation(num_augmented)\n",
        "    images_augmented = images_augmented[permuted_sequence]\n",
        "    labels_augmented = labels_augmented[permuted_sequence]\n",
        "\n",
        "    return images_augmented, labels_augmented\n"
      ],
      "id": "7ee7b917"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91f46696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255c832f-f712-44e8-ccdd-0d5f16a35d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Abyssinian_172.png', 'Abyssinian_173.png', 'Abyssinian_174.png', 'Abyssinian_175.png', 'Abyssinian_176.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:21<00:00,  7.04s/it]\n"
          ]
        }
      ],
      "source": [
        "if training_data_processing:\n",
        "    # Load and generate the resized training dataset and validation dataset\n",
        "    images_training, labels_training, images_validation, labels_validation = training_data_loading(path_training,\n",
        "                                                                                                   training_resize,\n",
        "                                                                                                   training_number)\n",
        "    # Generate the augmented training dataset\n",
        "    rotations = [0, 45, 90, 135] # the rotation angle\n",
        "\n",
        "    flips = ['original', np.flipud, np.fliplr] # 'original', np.flipud, np.fliplr\n",
        "\n",
        "    shifts = [(-16, 16)]\n",
        "\n",
        "    images_augmented, labels_augmented = training_data_augmentation(images_training,\n",
        "                                                                    labels_training,\n",
        "                                                                    rotations,\n",
        "                                                                    flips,\n",
        "                                                                    shifts,\n",
        "                                                                    training_resize)\n",
        "    # Save the augmented training dataset and resized validation dataset\n",
        "    # to your Google Drive or local file system\n",
        "    np.save(f'{path_data}images_training', images_augmented)\n",
        "    np.save(f'{path_data}labels_training', labels_augmented)\n",
        "    np.save(f'{path_data}images_validation', images_validation)\n",
        "    np.save(f'{path_data}labels_validation', labels_validation)\n",
        "elif not model_loading:\n",
        "    # Load the augmented training dataset and resized validation dataset\n",
        "    # from your Google Drive or local file system\n",
        "    images_augmented = np.load(f'{path_data}images_training.npy')\n",
        "    labels_augmented = np.load(f'{path_data}labels_training.npy')\n",
        "    images_validation = np.load(f'{path_data}images_validation.npy')\n",
        "    labels_validation = np.load(f'{path_data}labels_validation.npy')"
      ],
      "id": "91f46696"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ac8f763"
      },
      "source": [
        "# **Train the Instance of the Neural Network**"
      ],
      "id": "0ac8f763"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67068793"
      },
      "outputs": [],
      "source": [
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    def forward(self, outputs, targets, smooth=0):\n",
        "        \"\"\"\n",
        "        DiceBCELoss - Compute the Dice-BCE Loss.\n",
        "        Args:\n",
        "            outputs (tensor): output tensor\n",
        "            targets (tensor): target tensor\n",
        "        Returns:\n",
        "            dice_BCE_loss (tensor): the Dice-BCE Loss\n",
        "        \"\"\"\n",
        "        # Flatten output and target tensors\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # Compute the dice Loss\n",
        "        intersection = (outputs * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + smooth) / (outputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        # Compute the standard binary cross-entropy (BCE) loss\n",
        "        BCE_loss = self.bce_loss(outputs, targets)\n",
        "\n",
        "        dice_BCE_loss = dice_loss + BCE_loss\n",
        "\n",
        "        return dice_BCE_loss\n",
        "\n",
        "\n",
        "class BCEIoULoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(BCEIoULoss, self).__init__()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    def forward(self, outputs, targets, beta=0.6, alpha=0.25, gamma=3, smooth=0):\n",
        "        \"\"\"\n",
        "        BCEIoULoss - Compute the BCEIoULoss Loss.\n",
        "        Args:\n",
        "            outputs (tensor): output tensor\n",
        "            targets (tensor): target tensor\n",
        "        Returns:\n",
        "            BCE_IoU_loss (tensor): the BCE-IoU Loss\n",
        "        \"\"\"\n",
        "        # Flatten output and target tensors\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # Compute the intersection-over-union (IoU) loss\n",
        "        intersection = (outputs * targets).sum()\n",
        "        total = (outputs + targets).sum()\n",
        "        union = total - intersection\n",
        "        IoU_loss = 1 - (intersection + smooth) / (union + smooth)\n",
        "\n",
        "        # Compute the modified BCE loss\n",
        "        BCE_loss = self.bce_loss(outputs, targets)\n",
        "        BCE_exp = torch.exp(-BCE_loss)\n",
        "        modified_BCE_loss = alpha * (1 - BCE_exp) ** gamma * BCE_loss\n",
        "\n",
        "        BCE_IoU_loss = beta * modified_BCE_loss + (1 - beta) * IoU_loss\n",
        "\n",
        "        return BCE_IoU_loss"
      ],
      "id": "67068793"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "882daf1f"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          images_training,\n",
        "          labels_training,\n",
        "          images_validation,\n",
        "          labels_validation,\n",
        "          loss_func,\n",
        "          batch_size=8,\n",
        "          learning_rate=1e-3,\n",
        "          epochs=80,\n",
        "          model_validation=False,\n",
        "          cuda_available=True,\n",
        "          path_model = 'models/'):\n",
        "    \"\"\"\n",
        "    train - Train the instance of the neural network.\n",
        "    Args:\n",
        "        model (torch): the instance of the neural network\n",
        "        images_training, labels_training (numpy): the augmented training dataset\n",
        "        images_validation, labels_validation (numpy): the resized validation dataset\n",
        "        loss_func (class): the loss function\n",
        "        batch_size (int): the number of samples per batch to load (default: 8)\n",
        "        learning_rate (float): the learning rate (default: 1e-3)\n",
        "        epochs (int): the learning epochs (default: 80)\n",
        "        if_validation (bool): the flag indicating whether or not to implement validation (default: False)\n",
        "        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n",
        "    \"\"\"\n",
        "    # Use torch.utils.data to create a training_generator\n",
        "    images_training = torch.Tensor(images_training)\n",
        "    labels_training = torch.Tensor(labels_training)\n",
        "    training_set = TensorDataset(images_training, labels_training)\n",
        "    training_generator = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Use torch.utils.data to create a validation_generator\n",
        "    if model_validation and len(images_validation) > 0:\n",
        "        images_validation = torch.Tensor(images_validation)\n",
        "        labels_validation = torch.Tensor(labels_validation)\n",
        "        validation_set = TensorDataset(images_validation, labels_validation)\n",
        "        validation_generator = DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Implement Adam algorithm\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Decay the learning rate by gamma every step_size epochs.\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5, verbose=True)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # Training\n",
        "        print(f'\\n---------Training for Epoch {epoch + 1} starting:---------')\n",
        "        model.train()\n",
        "        loss_training = 0\n",
        "        # Loop over batches in an epoch using training_generator\n",
        "        for index, (inputs, labels) in enumerate(training_generator):\n",
        "            if cuda_available:\n",
        "                # Transfer to GPU\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_training += loss\n",
        "\n",
        "            if index % 20 == 0:\n",
        "                loss_item = loss.item()\n",
        "                print(f'→ Running_loss for Batch {index + 1}: {loss_item}')\n",
        "\n",
        "        print(f'\\033[1mTraining loss for Epoch {epoch + 1}: {loss_training}\\033[0m\\n')\n",
        "\n",
        "        if model_validation and len(images_validation) > 0:\n",
        "            # Validation\n",
        "            print(f'--------Validation for Epoch {epoch + 1} starting:--------')\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                loss_validation = 0\n",
        "                # Loop over batches in an epoch using validation_generator\n",
        "                for index, (inputs, labels) in enumerate(validation_generator):\n",
        "                    if cuda_available:\n",
        "                        # Transfer to GPU\n",
        "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss_validation += loss_func(outputs, labels)\n",
        "\n",
        "            print(f'\\033[1mValidation loss for Epoch {epoch + 1}: {loss_validation}\\033[0m\\n')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss_func,\n",
        "                }, path_model + 'model.model')"
      ],
      "id": "882daf1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33caad3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d245deaa-81b7-435b-a0c0-942adc7d3f66"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "images_training.shape = (36, 3, 384, 384)\n",
            "labels_training.shape = (36, 1, 384, 384)\n",
            "images_validation.shape = (2, 3, 384, 384)\n",
            "labels_validation.shape = (2, 1, 384, 384)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------Training for Epoch 1 starting:---------\n",
            "→ Running_loss for Batch 1: 0.2994975745677948\n",
            "\u001b[1mTraining loss for Epoch 1: 1.081300973892212\u001b[0m\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 1/4 [14:03<42:10, 843.35s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------Training for Epoch 2 starting:---------\n",
            "→ Running_loss for Batch 1: 0.13075652718544006\n"
          ]
        }
      ],
      "source": [
        "if model_training:\n",
        "    print(f\"\\nimages_training.shape = {images_augmented.shape}\")\n",
        "    print(f\"labels_training.shape = {labels_augmented.shape}\")\n",
        "    print(f\"images_validation.shape = {images_validation.shape}\")\n",
        "    print(f\"labels_validation.shape = {labels_validation.shape}\")\n",
        "\n",
        "    train(model,\n",
        "          images_augmented,\n",
        "          labels_augmented,\n",
        "          images_validation,\n",
        "          labels_validation,\n",
        "          loss_func=BCEIoULoss(), # BCEIoULoss(), DiceBCELoss(), nn.BCELoss()\n",
        "          batch_size=8,\n",
        "          learning_rate=1e-3,\n",
        "          epochs=4,\n",
        "          model_validation=model_validation,\n",
        "          cuda_available=cuda_available,\n",
        "          path_model=path_model)"
      ],
      "id": "33caad3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r7atglYKv4-"
      },
      "outputs": [],
      "source": [
        "if model_loading:\n",
        "    # Load the model from your Google Drive or local file system\n",
        "    checkpoint = torch.load(path_model + 'model.model')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "id": "4r7atglYKv4-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Evk4YfAyK7"
      },
      "source": [
        "# **Process the Testing Dataset and Create the Submission File**"
      ],
      "id": "A_Evk4YfAyK7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6e6fd77"
      },
      "outputs": [],
      "source": [
        "def testing_patch_extracting(input, trar=384, tesr=584):\n",
        "    \"\"\"\n",
        "    testing_patch_extracting - Divide each resized testing image into four patches, one at each corner.\n",
        "    Args:\n",
        "        input (numpy) - the resized testing image\n",
        "        trar (int) - the resolution of resized training images and the corresponding masks\n",
        "        tesr (int) - the resolution of resized testing images\n",
        "    Returns:\n",
        "        input_patches (numpy) - the four patches\n",
        "    \"\"\"\n",
        "    if tesr / 2 > trar:\n",
        "        raise AssertionError(\"training_resize is too small.\")\n",
        "\n",
        "    input_patches = np.empty(shape=(4, input.shape[2], trar, trar))\n",
        "    input_patches[0] = np.transpose(input[0:0+trar, 0:0+trar, :], (2, 0, 1))\n",
        "    input_patches[1] = np.transpose(input[0:0+trar, tesr-trar:tesr, :], (2, 0, 1))\n",
        "    input_patches[2] = np.transpose(input[tesr-trar:tesr, 0:0+trar, :], (2, 0, 1))\n",
        "    input_patches[3] = np.transpose(input[tesr-trar:tesr, tesr-trar:tesr, :], (2, 0, 1))\n",
        "\n",
        "    return input_patches\n",
        "\n",
        "\n",
        "def testing_patch_assembling(output_patches, trar=384, tesr=584):\n",
        "    \"\"\"\n",
        "    testing_patch_assembling - Merge the four masks into one resized mask.\n",
        "    Args:\n",
        "        output_patches (numpy) - the masks of the four patches\n",
        "        trar (int) - the resolution of resized training images and the corresponding masks\n",
        "        tesr (int) - the resolution of resized testing images\n",
        "    Returns:\n",
        "        output (numpy) - the resized mask\n",
        "    \"\"\"\n",
        "    # The extracting length\n",
        "    eL = int(tesr / 2)\n",
        "\n",
        "    output = np.empty(shape=(output_patches.shape[1], tesr, tesr))\n",
        "    output[:, 0:eL, 0:eL] = output_patches[0, :, 0:eL, 0:eL]\n",
        "    output[:, 0:eL, tesr-eL:tesr] = output_patches[1, :, 0:eL, trar-eL:trar]\n",
        "    output[:, tesr-eL:tesr, 0:eL] = output_patches[2, :, trar-eL:trar, 0:eL]\n",
        "    output[:, tesr-eL:tesr, tesr-eL:tesr] = output_patches[3, :, trar-eL:trar, trar-eL:trar]\n",
        "\n",
        "    return output\n",
        "\n",
        "def mask_to_submission(output, index):\n",
        "    \"\"\"\n",
        "    mask_to_submission - Convert the mask of each testing image into the submission format.\n",
        "    Args:\n",
        "        output (numpy) - the mask of the testing image\n",
        "        index (int) - the index of the testing image\n",
        "    Returns:\n",
        "        mask_submission (list) - the submission format of the mask\n",
        "    \"\"\"\n",
        "    mask_submission = []\n",
        "    for i in range(0, output.shape[0], 16):\n",
        "        for j in range(0, output.shape[1], 16):\n",
        "            prediction = 0\n",
        "            patch = output[j:j+16, i:i+16]\n",
        "            if np.mean(patch > 0.2) > 0.25:\n",
        "                prediction = 1\n",
        "            mask_submission.append([\"{:03d}_{}_{}\".format(index, i, j), prediction])\n",
        "    return mask_submission\n",
        "\n",
        "\n",
        "def submission_creating(model, path_testing='test_set_images/', training_resize=384, testing_resize=584, cuda_available=True):\n",
        "    \"\"\"\n",
        "    submission_creating - Load and generate the resized training dataset and validation dataset.\n",
        "    Args:\n",
        "        model (torch): the instance of the neural network\n",
        "        path_testing (str): the location in your Google Drive or local file system\n",
        "        training_resize (int): the resolution of resized training images and their corresponding masks (training pairs) (default: 384)\n",
        "        testing_resize (int): the resolution of resized testing images (default: 584)\n",
        "        cuda_available (bool): the flag indicating whether CUDA is available (default: True)\n",
        "    Returns:\n",
        "        submission (numpy): the final submission file\n",
        "    \"\"\"\n",
        "    submit_outputs = []\n",
        "    for index,filename in enumerate(os.listdir(path_testing)):\n",
        "        if filename.lower().endswith('png'):\n",
        "            image_path = os.path.join(path_testing, filename)\n",
        "\n",
        "            # Load a testing image using OpenCV\n",
        "            input = cv2.imread(image_path)\n",
        "            input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB).astype('float32') / 255\n",
        "\n",
        "            # Resize the testing image\n",
        "            input = resize(input, (testing_resize, testing_resize))\n",
        "\n",
        "            # Divide the resized testing image into four patches, one at each corner.\n",
        "            input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n",
        "            input_patches = torch.from_numpy(input_patches).float()\n",
        "\n",
        "            # Predict the mask of the four patches\n",
        "            if cuda_available:\n",
        "                output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n",
        "            else:\n",
        "                output_patches = model(input_patches).detach().numpy()\n",
        "\n",
        "            # Merge the four masks into one resized mask\n",
        "            output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n",
        "\n",
        "            # Restore the resized mask to the original resolution\n",
        "            output = resize(output, (608, 608))\n",
        "\n",
        "            # Convert the mask of the testing image into the submission format\n",
        "            submit_output = mask_to_submission(output, index)\n",
        "\n",
        "            submit_outputs.append(submit_output)\n",
        "\n",
        "    submission = np.concatenate(submit_outputs, axis=0)\n",
        "    submission = np.concatenate(([['id', 'prediction']], submission), axis=0)\n",
        "\n",
        "    return submission"
      ],
      "id": "f6e6fd77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcc3e105"
      },
      "outputs": [],
      "source": [
        "submission = submission_creating(model,\n",
        "                                 path_testing,\n",
        "                                 training_resize,\n",
        "                                 testing_resize,\n",
        "                                 cuda_available)\n",
        "\n",
        "np.savetxt(\"submit.csv\", submission, delimiter=\",\", fmt = '%s')\n",
        "if use_google_colab:\n",
        "    files.download('submit.csv')"
      ],
      "id": "fcc3e105"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24c4b4cb"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "for filename in os.listdir(path_testing):\n",
        "  # print(filename[-3:])\n",
        "  if filename[-3:] == 'png':\n",
        "    image_path = os.path.join(path_testing,filename)\n",
        "\n",
        "    # Load a testing image\n",
        "    input = np.array(Image.open(image_path)).astype('float32') / 255\n",
        "\n",
        "    # Resize the testing image\n",
        "    input = resize(input, (testing_resize, testing_resize))\n",
        "\n",
        "    # Divide the resized testing image into four patches, one at each corner.\n",
        "    input_patches = testing_patch_extracting(input, training_resize, testing_resize)\n",
        "    input_patches = torch.from_numpy(input_patches).float()\n",
        "\n",
        "    # Predict the mask of the four patches\n",
        "    if cuda_available:\n",
        "        output_patches = model(input_patches.cuda()).detach().cpu().numpy()\n",
        "    else:\n",
        "        output_patches = model(input_patches).detach().numpy()\n",
        "\n",
        "    # Merge the four masks into one resized mask\n",
        "    output = testing_patch_assembling(output_patches, training_resize, testing_resize)[0, :, :]\n",
        "\n",
        "    # Restore the resized mask to the original resolution\n",
        "    output = resize(output, (608, 608))\n",
        "\n",
        "    f, axs = plt.subplots(1, 2)\n",
        "    axs[0].imshow(input)\n",
        "    axs[1].imshow(output)"
      ],
      "id": "24c4b4cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buz2mE2DA5pL"
      },
      "outputs": [],
      "source": [
        "im = Image.fromarray(output * 255)\n",
        "im = im.convert(\"L\")\n",
        "im.save(\"mask.png\")\n",
        "if use_google_colab:\n",
        "    files.download('mask.png')"
      ],
      "id": "Buz2mE2DA5pL"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}